"""
XGBoost Killer Training Pipeline
Revolutionary approach to definitively beat XGBoost with RÂ² > 0.8760
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import json
import os
from datetime import datetime
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# XGBoost for comparison
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib

import sys
sys.path.append('../models')
from revolutionary_weather_stgat import create_revolutionary_model

class XGBoostKillerTrainer:
    """Advanced trainer specifically designed to ANNIHILATE XGBoost"""
    
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        
        # REVOLUTIONARY OPTIMIZATION STRATEGY
        # Multi-optimizer approach with different learning rates
        self.main_optimizer = optim.AdamW(
            model.parameters(),
            lr=0.0005,  # Conservative start
            weight_decay=0.001,  # Light regularization
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # Revolutionary learning rate scheduling
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.main_optimizer,
            T_0=10,  # Restart every 10 epochs
            T_mult=2,  # Double restart period each time
            eta_min=1e-6
        )
        
        # Advanced loss functions
        self.mse_loss = nn.MSELoss()
        self.l1_loss = nn.L1Loss()
        self.huber_loss = nn.HuberLoss(delta=0.05)
        self.smooth_l1_loss = nn.SmoothL1Loss()
        
        # Training tracking
        self.train_losses = []
        self.val_losses = []
        self.val_r2_scores = []
        self.best_val_loss = float('inf')
        self.best_r2 = -float('inf')
        self.patience_counter = 0
        self.best_model_state = None
        
        # XGBoost target tracking
        self.xgboost_target = 0.8760
        self.beaten_xgboost = False
        
    def compute_revolutionary_loss(self, predictions, targets, weather_logits, attention_dict):
        """Multi-component loss function engineered to beat XGBoost"""
        
        # Primary prediction losses with different characteristics
        mse_loss = self.mse_loss(predictions, targets)
        l1_loss = self.l1_loss(predictions, targets)
        huber_loss = self.huber_loss(predictions, targets)
        smooth_l1_loss = self.smooth_l1_loss(predictions, targets)
        
        # Combine multiple loss functions for robustness
        prediction_loss = (
            0.4 * mse_loss +
            0.2 * l1_loss + 
            0.2 * huber_loss +
            0.2 * smooth_l1_loss
        )
        
        # Auxiliary weather classification loss for better representations
        if weather_logits is not None:
            # Create pseudo-labels based on input features for weather classification
            batch_size = weather_logits.shape[0]
            # Simple weather pseudo-labeling (can be improved with real labels)
            weather_targets = torch.randint(0, 8, (batch_size,), device=weather_logits.device)
            weather_loss = F.cross_entropy(weather_logits, weather_targets)
        else:
            weather_loss = torch.tensor(0.0, device=predictions.device)
        
        # Expert diversity regularization
        expert_weights = attention_dict.get('expert_weights', None)
        if expert_weights is not None:
            # Encourage expert diversity
            expert_entropy = -torch.sum(expert_weights * torch.log(expert_weights + 1e-8), dim=1)
            expert_diversity_loss = -expert_entropy.mean()  # Negative to maximize entropy
        else:
            expert_diversity_loss = torch.tensor(0.0, device=predictions.device)
        
        # Temporal consistency regularization
        temporal_attention = attention_dict.get('temporal_attention', None)
        if temporal_attention is not None:
            # Encourage smooth temporal attention
            temporal_consistency_loss = torch.mean(torch.abs(temporal_attention[..., 1:] - temporal_attention[..., :-1]))
        else:
            temporal_consistency_loss = torch.tensor(0.0, device=predictions.device)
        
        # Total loss combination
        total_loss = (
            prediction_loss + 
            0.1 * weather_loss +
            0.05 * expert_diversity_loss +
            0.02 * temporal_consistency_loss
        )
        
        return total_loss, {
            'total_loss': total_loss.item(),
            'prediction_loss': prediction_loss.item(),
            'mse_loss': mse_loss.item(),
            'l1_loss': l1_loss.item(),
            'huber_loss': huber_loss.item(),
            'weather_loss': weather_loss.item(),
            'expert_diversity_loss': expert_diversity_loss.item(),
            'temporal_consistency_loss': temporal_consistency_loss.item()
        }
    
    def train_epoch(self, train_loader, epoch):
        """Revolutionary training epoch with advanced techniques"""
        self.model.train()
        total_losses = []
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1} - KILLING XGBoost')
        for batch_idx, (features, targets) in enumerate(pbar):
            features = features.to(self.device)
            targets = targets.to(self.device)
            
            # Forward pass
            predictions, weather_logits, attention_dict = self.model(features)
            loss, loss_components = self.compute_revolutionary_loss(
                predictions, targets, weather_logits, attention_dict
            )
            
            # Backward pass with gradient accumulation
            self.main_optimizer.zero_grad()
            loss.backward()
            
            # Revolutionary gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.main_optimizer.step()
            
            total_losses.append(loss.item())
            
            # Update progress
            if batch_idx % 25 == 0:
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'lr': f'{self.scheduler.get_last_lr()[0]:.6f}',
                    'target': f'>{self.xgboost_target:.4f}'
                })
        
        # Step scheduler
        self.scheduler.step()
        
        return np.mean(total_losses)
    
    def validate_epoch(self, val_loader):
        """Comprehensive validation with XGBoost beating detection"""
        self.model.eval()
        all_predictions = []
        all_targets = []
        val_losses = []
        
        with torch.no_grad():
            for features, targets in val_loader:
                features = features.to(self.device)
                targets = targets.to(self.device)
                
                predictions, weather_logits, attention_dict = self.model(features)
                loss, _ = self.compute_revolutionary_loss(
                    predictions, targets, weather_logits, attention_dict
                )
                
                val_losses.append(loss.item())
                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(targets.cpu().numpy())
        
        # Calculate comprehensive metrics
        predictions_array = np.concatenate(all_predictions, axis=0)
        targets_array = np.concatenate(all_targets, axis=0)
        
        # Flatten for sklearn metrics
        pred_flat = predictions_array.flatten()
        target_flat = targets_array.flatten()
        
        mse = mean_squared_error(target_flat, pred_flat)
        mae = mean_absolute_error(target_flat, pred_flat)
        r2 = r2_score(target_flat, pred_flat)
        rmse = np.sqrt(mse)
        
        return np.mean(val_losses), {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'r2': r2
        }
    
    def train(self, train_data, val_data, epochs=150, batch_size=128, patience=50):
        """Revolutionary training to DESTROY XGBoost"""
        X_train, y_train = train_data
        X_val, y_val = val_data
        
        print(f"ðŸ”¥ðŸ”¥ðŸ”¥ XGBOOST KILLER TRAINING ðŸ”¥ðŸ”¥ðŸ”¥")
        print(f"Training samples: {len(X_train):,}")
        print(f"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"ðŸŽ¯ TARGET: DESTROY XGBoost RÂ² = {self.xgboost_target:.4f}")
        print(f"ðŸš€ Revolutionary Architecture: Mixture of Experts + Advanced Fusion")
        
        # Create optimized data loaders
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                                 num_workers=4, pin_memory=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, 
                               num_workers=4, pin_memory=True)
        
        # Training loop
        for epoch in range(epochs):
            print(f"\n{'='*80}")
            print(f"ðŸ”¥ EPOCH {epoch + 1}/{epochs} - DESTROYING XGBOOST ðŸ”¥")
            print(f"Learning Rate: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # Train
            train_loss = self.train_epoch(train_loader, epoch)
            
            # Validate
            val_loss, val_metrics = self.validate_epoch(val_loader)
            
            # Track progress
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.val_r2_scores.append(val_metrics['r2'])
            
            # Check for improvement
            if val_metrics['r2'] > self.best_r2:
                self.best_r2 = val_metrics['r2']
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self.best_model_state = self.model.state_dict().copy()
                
                print(f"ðŸŽ¯ NEW BEST MODEL! RÂ²: {val_metrics['r2']:.4f}, RMSE: {val_metrics['rmse']:.4f}")
                
                # Check if we beat XGBoost
                if val_metrics['r2'] > self.xgboost_target:
                    print(f"ðŸ”¥ðŸ”¥ðŸ”¥ XGBOOST DESTROYED! RÂ² = {val_metrics['r2']:.4f} > {self.xgboost_target:.4f} ðŸ”¥ðŸ”¥ðŸ”¥")
                    self.beaten_xgboost = True
                else:
                    gap = self.xgboost_target - val_metrics['r2']
                    progress = (val_metrics['r2'] / self.xgboost_target) * 100
                    print(f"âš¡ Progress: {progress:.1f}% toward XGBoost, Gap: {gap:.4f}")
                
            else:
                self.patience_counter += 1
                print(f"â³ Patience: {self.patience_counter}/{patience}")
            
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss: {val_loss:.4f}, RÂ²: {val_metrics['r2']:.4f}, RMSE: {val_metrics['rmse']:.4f}")
            
            # Early stopping
            if self.patience_counter >= patience:
                print(f"ðŸ›‘ Early stopping at epoch {epoch + 1}")
                break
                
            # If we beat XGBoost, continue for a few more epochs to maximize performance
            if self.beaten_xgboost and self.patience_counter >= 10:
                print(f"ðŸ† XGBoost beaten! Stopping to prevent overfitting.")
                break
        
        # Load best model
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
        
        print(f"\nðŸ† REVOLUTIONARY TRAINING COMPLETED! ðŸ†")
        print(f"Best RÂ²: {self.best_r2:.4f}")
        print(f"Best RMSE: {np.sqrt(self.best_val_loss):.4f}")
        
        if self.best_r2 > self.xgboost_target:
            print("âœ… ðŸ”¥ðŸ”¥ðŸ”¥ XGBOOST SUCCESSFULLY DESTROYED! ðŸ”¥ðŸ”¥ðŸ”¥ âœ…")
        else:
            print("âŒ Need more revolutionary optimization...")
        
        return {
            'best_r2': self.best_r2,
            'best_rmse': np.sqrt(self.best_val_loss),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_r2_scores': self.val_r2_scores,
            'beaten_xgboost': self.beaten_xgboost
        }

def train_xgboost_baseline(X_train, y_train, X_test, y_test):
    """Train the XGBoost baseline we need to beat"""
    
    print("ðŸŒ³ Training XGBoost baseline to establish the target...")
    
    # Flatten data for XGBoost
    X_train_flat = X_train.reshape(X_train.shape[0], -1)
    X_test_flat = X_test.reshape(X_test.shape[0], -1)
    
    # Handle target flattening
    if len(y_train.shape) > 2:
        y_train_flat = y_train.mean(axis=(1, 2))
        y_test_flat = y_test.mean(axis=(1, 2))
    elif len(y_train.shape) == 2:
        y_train_flat = y_train.mean(axis=1)
        y_test_flat = y_test.mean(axis=1)
    else:
        y_train_flat = y_train.flatten()
        y_test_flat = y_test.flatten()
    
    print(f"XGBoost input shapes: X_train {X_train_flat.shape}, y_train {y_train_flat.shape}")
    
    # Optimized XGBoost
    xgb_model = xgb.XGBRegressor(
        n_estimators=300,  # More trees
        max_depth=10,      # Deeper trees
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        reg_alpha=0.1,     # L1 regularization
        reg_lambda=0.1     # L2 regularization
    )
    
    xgb_model.fit(X_train_flat, y_train_flat)
    xgb_pred = xgb_model.predict(X_test_flat)
    
    xgb_r2 = r2_score(y_test_flat, xgb_pred)
    xgb_rmse = np.sqrt(mean_squared_error(y_test_flat, xgb_pred))
    
    print(f"XGBoost Performance - RÂ²: {xgb_r2:.4f}, RMSE: {xgb_rmse:.4f}")
    
    return {
        'r2': xgb_r2,
        'rmse': xgb_rmse,
        'mae': mean_absolute_error(y_test_flat, xgb_pred)
    }

def run_xgboost_killer():
    """Main function to DESTROY XGBoost"""
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ðŸ”¥ðŸ”¥ðŸ”¥ XGBOOST KILLER MODE ACTIVATED ðŸ”¥ðŸ”¥ðŸ”¥")
    print(f"Device: {device}")
    
    # Load complete dataset
    print("ðŸ“‚ Loading COMPLETE dataset for MAXIMUM POWER...")
    X_train = np.load('../../data/processed/X_train.npy')
    y_train = np.load('../../data/processed/y_train.npy')
    X_val = np.load('../../data/processed/X_val.npy')
    y_val = np.load('../../data/processed/y_val.npy')
    X_test = np.load('../../data/processed/X_test.npy')
    y_test = np.load('../../data/processed/y_test.npy')
    
    print(f"Training: {X_train.shape[0]:,} samples")
    print(f"Validation: {X_val.shape[0]:,} samples")
    print(f"Test: {X_test.shape[0]:,} samples")
    
    # Train XGBoost baseline
    xgboost_results = train_xgboost_baseline(X_train, y_train, X_test, y_test)
    xgboost_target = xgboost_results['r2']
    
    print(f"\nðŸŽ¯ XGBOOST TARGET TO DESTROY: RÂ² = {xgboost_target:.4f}")
    
    # Create revolutionary model
    print("ðŸ—ï¸ Creating REVOLUTIONARY Weather-Aware STGAT...")
    revolutionary_model = create_revolutionary_model(X_train.shape[-1], device)
    
    total_params = sum(p.numel() for p in revolutionary_model.parameters())
    print(f"Revolutionary Model Parameters: {total_params:,}")
    
    # Create XGBoost killer trainer
    killer_trainer = XGBoostKillerTrainer(
        model=revolutionary_model,
        device=device
    )
    killer_trainer.xgboost_target = xgboost_target
    
    # DESTROY XGBoost
    print("ðŸš€ DEPLOYING XGBOOST KILLER...")
    training_results = killer_trainer.train(
        train_data=(X_train, y_train),
        val_data=(X_val, y_val),
        epochs=150,
        batch_size=128,
        patience=50
    )
    
    # Final evaluation on test set
    print("\nðŸ“Š FINAL DESTRUCTION ASSESSMENT")
    print("="*80)
    
    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))
    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)
    
    killer_trainer.model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for features, targets in test_loader:
            features = features.to(device)
            predictions, _, _ = killer_trainer.model(features)
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
    
    # Calculate final metrics
    final_predictions = np.concatenate(all_predictions, axis=0).flatten()
    final_targets = np.concatenate(all_targets, axis=0).flatten()
    
    final_results = {
        'revolutionary_weather_stgat': {
            'rmse': np.sqrt(mean_squared_error(final_targets, final_predictions)),
            'mae': mean_absolute_error(final_targets, final_predictions),
            'r2': r2_score(final_targets, final_predictions)
        },
        'xgboost': xgboost_results
    }
    
    print("ðŸ† FINAL DESTRUCTION RESULTS")
    print("="*80)
    
    our_r2 = final_results['revolutionary_weather_stgat']['r2']
    xgb_r2 = final_results['xgboost']['r2']
    
    print(f"Revolutionary Weather-Aware STGAT: RÂ² = {our_r2:.4f}")
    print(f"XGBoost (TARGET):               RÂ² = {xgb_r2:.4f}")
    
    if our_r2 > xgb_r2:
        improvement = ((our_r2 - xgb_r2) / xgb_r2) * 100
        print(f"\nðŸ”¥ðŸ”¥ðŸ”¥ XGBOOST DESTROYED! ðŸ”¥ðŸ”¥ðŸ”¥")
        print(f"ðŸš€ IMPROVEMENT: +{improvement:.2f}% over XGBoost!")
        print("âœ… MISSION ACCOMPLISHED!")
    else:
        gap = xgb_r2 - our_r2
        print(f"\nâŒ Still {gap:.4f} RÂ² points behind XGBoost")
        print("ðŸ”„ Need more revolutionary optimization...")
    
    # Save results
    os.makedirs('../../results/xgboost_killer', exist_ok=True)
    
    complete_results = {
        'final_comparison': final_results,
        'training_history': training_results,
        'model_parameters': total_params,
        'xgboost_destroyed': our_r2 > xgb_r2,
        'improvement_over_xgboost': ((our_r2 - xgb_r2) / xgb_r2) * 100 if xgb_r2 > 0 else 0,
        'timestamp': datetime.now().isoformat()
    }
    
    with open('../../results/xgboost_killer/destruction_results.json', 'w') as f:
        json.dump(complete_results, f, indent=2)
    
    return complete_results

if __name__ == "__main__":
    results = run_xgboost_killer()